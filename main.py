#!/usr/bin/env python3
"""
ì›¹ í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ëª…ë ¹ì¤„ì—ì„œ ì›¹ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path
from typing import List

from web_crawler import WebCrawler, create_crawler_from_config_file


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="ì›¹ì‚¬ì´íŠ¸ì—ì„œ íŒŒì¼ì„ í¬ë¡¤ë§í•˜ê³  ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì œ:
  python main.py https://example.com
  python main.py https://example.com -t documents images
  python main.py https://example.com -o ./my_downloads -d 2
  python main.py https://example.com --find-only
  python main.py -c config.json https://example.com
        """
    )
    
    parser.add_argument(
        'urls',
        nargs='+',
        help='í¬ë¡¤ë§í•  URL ëª©ë¡'
    )
    
    parser.add_argument(
        '-t', '--file-types',
        nargs='*',
        choices=['documents', 'images', 'videos', 'audio', 'archives', 'data', 'executables', 'others'],
        default=['documents', 'images'],
        help='ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ íƒ€ì… (ê¸°ë³¸ê°’: documents images)'
    )
    
    parser.add_argument(
        '-e', '--extensions',
        nargs='*',
        help='ì‚¬ìš©ì ì •ì˜ íŒŒì¼ í™•ì¥ì (ì˜ˆ: .txt .log)'
    )
    
    parser.add_argument(
        '-o', '--output',
        default='./downloads',
        help='ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: ./downloads)'
    )
    
    parser.add_argument(
        '-d', '--depth',
        type=int,
        default=1,
        help='í¬ë¡¤ë§ ê¹Šì´ (ê¸°ë³¸ê°’: 1)'
    )
    
    parser.add_argument(
        '-c', '--config',
        help='JSON ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--max-concurrent',
        type=int,
        default=5,
        help='ìµœëŒ€ ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=30,
        help='íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 30)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=1.0,
        help='ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 1.0)'
    )
    
    parser.add_argument(
        '--find-only',
        action='store_true',
        help='íŒŒì¼ ë§í¬ë§Œ ì°¾ê³  ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ'
    )
    
    parser.add_argument(
        '--sync',
        action='store_true',
        help='ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ (ë¹„ë™ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” í™˜ê²½ìš©)'
    )
    
    parser.add_argument(
        '--no-metadata',
        action='store_true',
        help='ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ ì•ˆí•¨'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='ìµœì†Œí•œì˜ ì¶œë ¥ë§Œ í‘œì‹œ'
    )
    
    return parser.parse_args()


def setup_crawler_config(args) -> dict:
    """ëª…ë ¹ì¤„ ì¸ìë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ë¡¤ëŸ¬ ì„¤ì • ìƒì„±"""
    config = {}
    
    if args.config:
        # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except Exception as e:
            print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì„¤ì • ë®ì–´ì“°ê¸°
    config.update({
        'download_dir': args.output,
        'max_crawl_depth': args.depth,
        'max_concurrent_downloads': args.max_concurrent,
        'timeout': args.timeout,
        'delay_between_requests': args.delay,
        'file_types': args.file_types,
        'save_metadata': not args.no_metadata,
        'log_level': 'DEBUG' if args.verbose else 'ERROR' if args.quiet else 'INFO'
    })
    
    if args.extensions:
        config['custom_extensions'] = set(args.extensions)
    
    return config


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()
    
    # í¬ë¡¤ëŸ¬ ì„¤ì •
    config = setup_crawler_config(args)
    crawler = WebCrawler(config)
    
    print(f"ğŸ•·ï¸  ì›¹ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ğŸ“ ëŒ€ìƒ URL: {', '.join(args.urls)}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í„°ë¦¬: {args.output}")
    print(f"ğŸ“‚ íŒŒì¼ íƒ€ì…: {', '.join(args.file_types)}")
    
    if args.extensions:
        print(f"ğŸ”§ ì‚¬ìš©ì ì •ì˜ í™•ì¥ì: {', '.join(args.extensions)}")
    
    print(f"ğŸ” í¬ë¡¤ë§ ê¹Šì´: {args.depth}")
    print("-" * 50)
    
    try:
        if args.find_only:
            # íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸°
            print("ğŸ” íŒŒì¼ ë§í¬ íƒì§€ ëª¨ë“œ")
            
            file_links = await crawler.find_files_only(
                urls=args.urls,
                file_types=args.file_types,
                custom_extensions=set(args.extensions) if args.extensions else None
            )
            
            print("\nğŸ“‹ ë°œê²¬ëœ íŒŒì¼ ë§í¬:")
            total_files = 0
            for file_type, links in file_links.items():
                if links:
                    print(f"\n{file_type.upper()}:")
                    for i, link in enumerate(links, 1):
                        print(f"  {i:3d}. {link}")
                    total_files += len(links)
            
            print(f"\nì´ {total_files}ê°œ íŒŒì¼ ë°œê²¬")
            
            # ë§í¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            output_file = Path(args.output) / "found_links.json"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(file_links, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ë§í¬ ëª©ë¡ ì €ì¥: {output_file}")
            
        else:
            # í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ
            if args.sync:
                result = crawler.crawl_and_download_sync(
                    urls=args.urls,
                    file_types=args.file_types,
                    custom_extensions=set(args.extensions) if args.extensions else None,
                    output_dir=args.output
                )
            else:
                result = await crawler.crawl_and_download(
                    urls=args.urls,
                    file_types=args.file_types,
                    custom_extensions=set(args.extensions) if args.extensions else None,
                    output_dir=args.output
                )
            
            if result['success']:
                print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            else:
                print("âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                sys.exit(1)
                
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main())