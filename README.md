# ì›¹ í¬ë¡¤ëŸ¬ (Web Crawler)

ì›¹ì‚¬ì´íŠ¸ì—ì„œ íŒŒì¼ ë§í¬ë¥¼ ìë™ìœ¼ë¡œ íƒì§€í•˜ê³  ë‹¤ìš´ë¡œë“œí•˜ëŠ” Python ì›¹ í¬ë¡¤ëŸ¬ì…ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ğŸ” ìŠ¤ë§ˆíŠ¸ íŒŒì¼ íƒì§€**: HTML íƒœê·¸ì™€ HTTP í—¤ë”ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ ìë™ íƒì§€
- **ğŸ“ ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›**: ë¬¸ì„œ, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, ì••ì¶• íŒŒì¼ ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
- **âš¡ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ**: ë™ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
- **ğŸ”„ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œë„
- **ğŸ“Š ìƒì„¸í•œ í†µê³„**: í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ ì§„í–‰ìƒí™©ê³¼ ê²°ê³¼ í†µê³„
- **âš™ï¸ ìœ ì—°í•œ ì„¤ì •**: JSON ì„¤ì • íŒŒì¼ ë° ëª…ë ¹ì¤„ ì˜µì…˜ ì§€ì›
- **ğŸ“ ë©”íƒ€ë°ì´í„° ì €ì¥**: í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥

## ğŸ“¦ ì„¤ì¹˜

### ë°©ë²• 1: ê¸°ë³¸ ì„¤ì¹˜

1. **ì €ì¥ì†Œ í´ë¡ **
```bash
git clone <repository-url>
cd webcrawler
```

2. **ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
pip install -r requirements.txt
```

### ë°©ë²• 2: ê°€ìƒí™˜ê²½ ì‚¬ìš© (ê¶Œì¥)

1. **ì €ì¥ì†Œ í´ë¡ **
```bash
git clone <repository-url>
cd webcrawler
```

2. **ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”**
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

3. **ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
pip install -r requirements.txt
```

4. **ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”** (ì‚¬ìš© ì¢…ë£Œ ì‹œ)
```bash
deactivate
```

> **ğŸ’¡ íŒ**: ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•˜ë©´ ì‹œìŠ¤í…œ Python íŒ¨í‚¤ì§€ì™€ ë¶„ë¦¬ëœ ë…ë¦½ì ì¸ í™˜ê²½ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŒ¨í‚¤ì§€ ë²„ì „ ì¶©ëŒì„ ë°©ì§€í•˜ê³  ê¹”ë”í•œ ê°œë°œ í™˜ê²½ì„ ìœ ì§€í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

## ğŸ› ï¸ ì‚¬ìš©ë²•

### ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤

#### ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
# ê¸°ë³¸ í¬ë¡¤ë§ (ë¬¸ì„œì™€ ì´ë¯¸ì§€)
python main.py https://example.com

# íŠ¹ì • íŒŒì¼ íƒ€ì…ë§Œ ë‹¤ìš´ë¡œë“œ
python main.py https://example.com -t documents archives

# ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì§€ì •
python main.py https://example.com -o ./my_downloads

# í¬ë¡¤ë§ ê¹Šì´ ì„¤ì •
python main.py https://example.com -d 2

# íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸° (ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
python main.py https://example.com --find-only
```

#### ê³ ê¸‰ ì˜µì…˜
```bash
# ì‚¬ìš©ì ì •ì˜ í™•ì¥ì í¬í•¨
python main.py https://example.com -e .log .cfg .ini

# ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°ì ˆ
python main.py https://example.com --max-concurrent 10

# íƒ€ì„ì•„ì›ƒ ì„¤ì •
python main.py https://example.com --timeout 60

# ìš”ì²­ ê°„ ì§€ì—° ì„¤ì •
python main.py https://example.com --delay 2.0

# ì„¤ì • íŒŒì¼ ì‚¬ìš©
python main.py -c config.json https://example.com

# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
python main.py https://example.com --verbose

# ë™ê¸° ë°©ì‹ ì‹¤í–‰
python main.py https://example.com --sync
```

### Python ì½”ë“œì—ì„œ ì‚¬ìš©

#### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
import asyncio
from web_crawler import WebCrawler

async def main():
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = WebCrawler()
    
    # í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ
    result = await crawler.crawl_and_download(
        urls=["https://example.com"],
        file_types=["documents", "images"],
        output_dir="./downloads"
    )
    
    print(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼: {result['stats']['files_downloaded']}ê°œ")

asyncio.run(main())
```

#### ì‚¬ìš©ì ì •ì˜ ì„¤ì •
```python
from web_crawler import WebCrawler

# ì‚¬ìš©ì ì •ì˜ ì„¤ì •
config = {
    'download_dir': './custom_downloads',
    'max_concurrent_downloads': 10,
    'max_crawl_depth': 2,
    'file_types': ['documents', 'archives'],
    'custom_extensions': {'.log', '.cfg'},
    'delay_between_requests': 0.5
}

crawler = WebCrawler(config)

# ë™ê¸° ë°©ì‹ ì‹¤í–‰
result = crawler.crawl_and_download_sync(
    urls=["https://example.com"]
)
```

#### íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸°
```python
async def find_files():
    crawler = WebCrawler()
    
    # íŒŒì¼ ë§í¬ë§Œ íƒì§€ (ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
    file_links = await crawler.find_files_only(
        urls=["https://example.com"],
        file_types=["documents", "images", "videos"]
    )
    
    for file_type, links in file_links.items():
        print(f"{file_type}: {len(links)}ê°œ íŒŒì¼")
```

## ğŸ“‚ ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹

| ì¹´í…Œê³ ë¦¬ | í™•ì¥ì |
|---------|--------|
| **documents** | .pdf, .doc, .docx, .txt, .rtf, .odt |
| **images** | .jpg, .jpeg, .png, .gif, .bmp, .svg, .webp |
| **videos** | .mp4, .avi, .mov, .wmv, .flv, .webm, .mkv |
| **audio** | .mp3, .wav, .flac, .aac, .ogg, .wma |
| **archives** | .zip, .rar, .tar, .gz, .7z, .bz2 |
| **data** | .json, .xml, .csv, .xls, .xlsx |
| **executables** | .exe, .msi, .dmg, .deb, .rpm |
| **others** | .iso, .torrent, .apk |

## âš™ï¸ ì„¤ì • ì˜µì…˜

### config.json ì˜ˆì œ
```json
{
  "download_dir": "./downloads",
  "max_concurrent_downloads": 5,
  "max_crawl_depth": 1,
  "timeout": 30,
  "retry_count": 3,
  "chunk_size": 8192,
  "file_types": ["documents", "images", "videos"],
  "custom_extensions": [],
  "same_domain_only": true,
  "delay_between_requests": 1,
  "enable_logging": true,
  "log_level": "INFO",
  "save_metadata": true,
  "metadata_file": "crawl_metadata.json"
}
```

### ì„¤ì • ì˜µì…˜ ì„¤ëª…

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `download_dir` | ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ | `"./downloads"` |
| `max_concurrent_downloads` | ìµœëŒ€ ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜ | `5` |
| `max_crawl_depth` | í¬ë¡¤ë§ ê¹Šì´ | `1` |
| `timeout` | íƒ€ì„ì•„ì›ƒ (ì´ˆ) | `30` |
| `retry_count` | ì¬ì‹œë„ íšŸìˆ˜ | `3` |
| `chunk_size` | ë‹¤ìš´ë¡œë“œ ì²­í¬ í¬ê¸° | `8192` |
| `file_types` | ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ íƒ€ì… | `["documents", "images"]` |
| `custom_extensions` | ì‚¬ìš©ì ì •ì˜ í™•ì¥ì | `[]` |
| `delay_between_requests` | ìš”ì²­ ê°„ ì§€ì—° (ì´ˆ) | `1` |
| `enable_logging` | ë¡œê¹… í™œì„±í™” | `true` |
| `log_level` | ë¡œê·¸ ë ˆë²¨ | `"INFO"` |
| `save_metadata` | ë©”íƒ€ë°ì´í„° ì €ì¥ | `true` |

## ğŸ“Š ì¶œë ¥ ë° ê²°ê³¼

### í¬ë¡¤ë§ í†µê³„
í¬ë¡¤ë§ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ í†µê³„ê°€ í‘œì‹œë©ë‹ˆë‹¤:

```
ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
==================================================
ğŸŒ í¬ë¡¤ë§í•œ URL ìˆ˜: 1
ğŸ“ ë°œê²¬í•œ íŒŒì¼ ìˆ˜: 25
â¬‡ï¸  ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ ìˆ˜: 23
ğŸ’¾ ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: 15.7 MB
â±ï¸  ì†Œìš” ì‹œê°„: 45.32ì´ˆ

ğŸ“‚ íŒŒì¼ íƒ€ì…ë³„ ë°œê²¬ ìˆ˜:
   documents: 8ê°œ
   images: 15ê°œ
   archives: 2ê°œ
==================================================
```

### ë©”íƒ€ë°ì´í„° íŒŒì¼
í¬ë¡¤ë§ ê²°ê³¼ëŠ” JSON í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤:

```json
{
  "crawl_info": {
    "timestamp": "2024-01-15T10:30:00",
    "stats": {
      "urls_crawled": 1,
      "files_found": 25,
      "files_downloaded": 23
    }
  },
  "found_links": {
    "documents": ["url1", "url2"],
    "images": ["url3", "url4"]
  },
  "download_results": [
    {
      "url": "https://example.com/file.pdf",
      "success": true,
      "filename": "file.pdf",
      "size": 1024000
    }
  ]
}
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### ì‚¬ìš©ì ì •ì˜ í™•ì¥ì ì¶”ê°€
```python
crawler = WebCrawler()
crawler.add_custom_extensions(['log', 'cfg', 'ini'])
```

### ì„¤ì • íŒŒì¼ì—ì„œ í¬ë¡¤ëŸ¬ ìƒì„±
```python
from web_crawler import create_crawler_from_config_file

crawler = create_crawler_from_config_file("config.json")
```

### ë¹ ë¥¸ í¬ë¡¤ë§ í•¨ìˆ˜
```python
from web_crawler import quick_crawl_sync

result = quick_crawl_sync(
    url="https://example.com",
    file_types=["documents"],
    output_dir="./downloads"
)
```

## ğŸ“ ì˜ˆì œ ì½”ë“œ

`example.py` íŒŒì¼ì—ì„œ ë‹¤ì–‘í•œ ì‚¬ìš© ì˜ˆì œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
python example.py
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ë¡œë´‡ ë°°ì œ í‘œì¤€**: robots.txtë¥¼ í™•ì¸í•˜ê³  ì›¹ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ì •ì±…ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
2. **ìš”ì²­ ì œí•œ**: ì„œë²„ì— ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šë„ë¡ ì ì ˆí•œ ì§€ì—° ì‹œê°„ì„ ì„¤ì •í•˜ì„¸ìš”.
3. **ì €ì‘ê¶Œ**: ë‹¤ìš´ë¡œë“œí•˜ëŠ” íŒŒì¼ì˜ ì €ì‘ê¶Œê³¼ ì‚¬ìš© ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.
4. **ë²•ì  ì±…ì„**: ì›¹ í¬ë¡¤ë§ ì‹œ í•´ë‹¹ êµ­ê°€ì˜ ë²•ë¥ ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

- Python 3.7+
- requests
- beautifulsoup4
- lxml
- aiohttp
- aiofiles
- tqdm

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **SSL ì¸ì¦ì„œ ì˜¤ë¥˜**
   ```python
   import ssl
   ssl._create_default_https_context = ssl._create_unverified_context
   ```

2. **ì¸ì½”ë”© ë¬¸ì œ**
   - ìë™ìœ¼ë¡œ ì¸ì½”ë”©ì„ ê°ì§€í•˜ì§€ë§Œ, ë¬¸ì œê°€ ìˆì„ ê²½ìš° ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥

3. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”**
   - `chunk_size`ë¥¼ ì¡°ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆ
   - `max_concurrent_downloads`ë¥¼ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! ì´ìŠˆë¥¼ ë³´ê³ í•˜ê±°ë‚˜ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ ì œì¶œí•´ ì£¼ì„¸ìš”.

## ğŸ“§ ë¬¸ì˜

ì§ˆë¬¸ì´ë‚˜ ì œì•ˆì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”.