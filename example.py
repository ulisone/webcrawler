#!/usr/bin/env python3
"""
ì›¹ í¬ë¡¤ëŸ¬ ì‚¬ìš© ì˜ˆì œ
ë‹¤ì–‘í•œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤.
"""

import asyncio
from web_crawler import WebCrawler, quick_crawl_sync, create_crawler_from_config_file


async def example_basic_crawling():
    """ê¸°ë³¸ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("=== ê¸°ë³¸ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    # ê°„ë‹¨í•œ í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = WebCrawler()
    
    # ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ë¬¸ì„œì™€ ì´ë¯¸ì§€ë§Œ)
    result = await crawler.crawl_and_download(
        urls=["https://example.com"],
        file_types=["documents", "images"],
        output_dir="./example_downloads"
    )
    
    print(f"ë°œê²¬ëœ íŒŒì¼: {result['stats']['files_found']}ê°œ")
    print(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼: {result['stats']['files_downloaded']}ê°œ")


async def example_custom_config():
    """ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì˜ˆì œ"""
    print("\n=== ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì˜ˆì œ ===")
    
    # ì‚¬ìš©ì ì •ì˜ ì„¤ì •
    config = {
        'download_dir': './custom_downloads',
        'max_concurrent_downloads': 10,
        'max_crawl_depth': 2,
        'file_types': ['documents', 'archives'],
        'custom_extensions': {'.log', '.cfg'},
        'delay_between_requests': 0.5
    }
    
    crawler = WebCrawler(config)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    result = await crawler.crawl_and_download(
        urls=["https://example.com/downloads"]
    )
    
    print("ì‚¬ìš©ì ì •ì˜ ì„¤ì •ìœ¼ë¡œ í¬ë¡¤ë§ ì™„ë£Œ")


async def example_find_only():
    """íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸° ì˜ˆì œ"""
    print("\n=== íŒŒì¼ ë§í¬ íƒì§€ ì˜ˆì œ ===")
    
    crawler = WebCrawler()
    
    # íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸° (ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
    file_links = await crawler.find_files_only(
        urls=["https://example.com"],
        file_types=["documents", "images", "videos"]
    )
    
    print("ë°œê²¬ëœ íŒŒì¼ ë§í¬:")
    for file_type, links in file_links.items():
        if links:
            print(f"\n{file_type}: {len(links)}ê°œ")
            for link in links[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"  - {link}")
            if len(links) > 3:
                print(f"  ... ë° {len(links) - 3}ê°œ ë”")


async def example_download_from_list():
    """URL ëª©ë¡ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ"""
    print("\n=== URL ëª©ë¡ ë‹¤ìš´ë¡œë“œ ì˜ˆì œ ===")
    
    crawler = WebCrawler()
    
    # ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ URL ëª©ë¡
    file_urls = [
        "https://example.com/file1.pdf",
        "https://example.com/file2.jpg",
        "https://example.com/file3.zip"
    ]
    
    # ì§ì ‘ ë‹¤ìš´ë¡œë“œ
    results = await crawler.download_files_from_list(
        file_urls=file_urls,
        output_dir="./direct_downloads"
    )
    
    successful = [r for r in results if r['success']]
    print(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(successful)}/{len(file_urls)}ê°œ")


def example_sync_crawling():
    """ë™ê¸° ë°©ì‹ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n=== ë™ê¸° ë°©ì‹ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¹ ë¥¸ í¬ë¡¤ë§
    result = quick_crawl_sync(
        url="https://example.com",
        file_types=["documents"],
        output_dir="./sync_downloads"
    )
    
    print("ë™ê¸° ë°©ì‹ í¬ë¡¤ë§ ì™„ë£Œ")


def example_config_file():
    """ì„¤ì • íŒŒì¼ ì‚¬ìš© ì˜ˆì œ"""
    print("\n=== ì„¤ì • íŒŒì¼ ì‚¬ìš© ì˜ˆì œ ===")
    
    # ì„¤ì • íŒŒì¼ì—ì„œ í¬ë¡¤ëŸ¬ ìƒì„±
    try:
        crawler = create_crawler_from_config_file("config.json")
        
        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
        result = crawler.crawl_and_download_sync(
            urls=["https://example.com"]
        )
        
        print("ì„¤ì • íŒŒì¼ ê¸°ë°˜ í¬ë¡¤ë§ ì™„ë£Œ")
        
    except Exception as e:
        print(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")


def example_file_types():
    """ì§€ì›í•˜ëŠ” íŒŒì¼ íƒ€ì… í™•ì¸"""
    print("\n=== ì§€ì›í•˜ëŠ” íŒŒì¼ íƒ€ì… ===")
    
    crawler = WebCrawler()
    file_types = crawler.get_supported_file_types()
    
    for category, extensions in file_types.items():
        print(f"\n{category}:")
        print(f"  {', '.join(extensions)}")


async def example_advanced_usage():
    """ê³ ê¸‰ ì‚¬ìš©ë²• ì˜ˆì œ"""
    print("\n=== ê³ ê¸‰ ì‚¬ìš©ë²• ì˜ˆì œ ===")
    
    # ê³ ê¸‰ ì„¤ì •
    config = {
        'download_dir': './advanced_downloads',
        'max_concurrent_downloads': 8,
        'max_crawl_depth': 3,
        'timeout': 60,
        'retry_count': 5,
        'chunk_size': 16384,
        'save_metadata': True,
        'enable_logging': True,
        'log_level': 'DEBUG'
    }
    
    crawler = WebCrawler(config)
    
    # ì‚¬ìš©ì ì •ì˜ í™•ì¥ì ì¶”ê°€
    crawler.add_custom_extensions(['log', 'cfg', 'ini'])
    
    # ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
    urls = [
        "https://example.com/downloads",
        "https://example.org/files",
        "https://example.net/resources"
    ]
    
    result = await crawler.crawl_and_download(
        urls=urls,
        file_types=["documents", "archives", "data"],
        output_dir="./multi_site_downloads"
    )
    
    print("ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ")


async def main():
    """ëª¨ë“  ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ•·ï¸  ì›¹ í¬ë¡¤ëŸ¬ ì˜ˆì œ ì‹œì‘\n")
    
    # ì£¼ì˜: ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ URLì„ ë³€ê²½í•˜ì„¸ìš”
    print("âš ï¸  ì£¼ì˜: ì˜ˆì œëŠ” ê°€ìƒì˜ URLì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URLë¡œ ë³€ê²½í•˜ì„¸ìš”.\n")
    
    try:
        # ì˜ˆì œë“¤ ì‹¤í–‰ (ì‹¤ì œ URLì´ ì—†ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬)
        # await example_basic_crawling()
        # await example_custom_config()
        # await example_find_only()
        # await example_download_from_list()
        
        # ë™ê¸° ì˜ˆì œë“¤
        # example_sync_crawling()
        # example_config_file()
        
        # ì •ë³´ í‘œì‹œ ì˜ˆì œ (ì‹¤ì œ ë™ì‘)
        example_file_types()
        
        # await example_advanced_usage()
        
        print("\nâœ… ëª¨ë“  ì˜ˆì œ ì½”ë“œ í™•ì¸ ì™„ë£Œ")
        print("\nì‹¤ì œ ì‚¬ìš©ë²•:")
        print("1. main.pyë¥¼ ì‚¬ìš©í•œ ëª…ë ¹ì¤„ ì‹¤í–‰:")
        print("   python main.py https://example.com -t documents images")
        print("\n2. ì½”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©:")
        print("   from web_crawler import WebCrawler")
        print("   crawler = WebCrawler()")
        print("   result = await crawler.crawl_and_download(['https://example.com'])")
        
    except Exception as e:
        print(f"âŒ ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())