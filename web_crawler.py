"""
ì›¹ í¬ë¡¤ëŸ¬ ë©”ì¸ ì—”ì§„
ì›¹ì‚¬ì´íŠ¸ì—ì„œ íŒŒì¼ ë§í¬ë¥¼ ì°¾ê³  ë‹¤ìš´ë¡œë“œí•˜ëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional, Set, Any
from datetime import datetime
import time

from link_detector import LinkDetector
from file_downloader import FileDownloader


class WebCrawler:
    """ì›¹ í¬ë¡¤ëŸ¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        WebCrawler ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ì„¤ì •
        self.config = {
            'download_dir': './downloads',
            'max_concurrent_downloads': 5,
            'max_crawl_depth': 1,
            'timeout': 30,
            'retry_count': 3,
            'chunk_size': 8192,
            'file_types': ['documents', 'images', 'videos', 'audio', 'archives'],
            'custom_extensions': set(),
            'same_domain_only': True,
            'respect_robots_txt': False,
            'delay_between_requests': 1,
            'enable_logging': True,
            'log_level': 'INFO',
            'save_metadata': True,
            'metadata_file': 'crawl_metadata.json'
        }
        
        # ì‚¬ìš©ì ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if config:
            self.config.update(config)
        
        # ë¡œê¹… ì„¤ì •
        if self.config['enable_logging']:
            self._setup_logging()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.link_detector = LinkDetector()
        self.file_downloader = FileDownloader(
            download_dir=self.config['download_dir'],
            max_concurrent=self.config['max_concurrent_downloads'],
            chunk_size=self.config['chunk_size'],
            timeout=self.config['timeout'],
            retry_count=self.config['retry_count']
        )
        
        self.logger = logging.getLogger(__name__)
        
        # í¬ë¡¤ë§ í†µê³„
        self.crawl_stats = {
            'start_time': None,
            'end_time': None,
            'urls_crawled': 0,
            'files_found': 0,
            'files_downloaded': 0,
            'total_download_size': 0,
            'errors': []
        }
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata = {
            'crawl_info': {},
            'found_links': {},
            'download_results': [],
            'config': self.config.copy()
        }
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_level = getattr(logging, self.config['log_level'].upper(), logging.INFO)
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('web_crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    async def crawl_and_download(self, 
                               urls: List[str], 
                               file_types: List[str] = None,
                               custom_extensions: Set[str] = None,
                               output_dir: str = None) -> Dict[str, Any]:
        """
        ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ
        
        Args:
            urls: í¬ë¡¤ë§í•  URL ëª©ë¡
            file_types: ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ íƒ€ì… ëª©ë¡
            custom_extensions: ì‚¬ìš©ì ì •ì˜ í™•ì¥ì
            output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
            
        Returns:
            í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ ê²°ê³¼
        """
        self.crawl_stats['start_time'] = time.time()
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        if file_types:
            self.config['file_types'] = file_types
        if custom_extensions:
            self.config['custom_extensions'] = custom_extensions
        if output_dir:
            self.config['download_dir'] = output_dir
            self.file_downloader.download_dir = Path(output_dir)
            self.file_downloader.download_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL")
        
        # ëª¨ë“  íŒŒì¼ ë§í¬ ìˆ˜ì§‘
        all_file_links = {}
        
        for url in urls:
            try:
                self.logger.info(f"URL í¬ë¡¤ë§ ì¤‘: {url}")
                
                # íŒŒì¼ ë§í¬ ì°¾ê¸°
                file_links = self.link_detector.find_file_links(
                    url=url,
                    file_types=self.config['file_types'],
                    custom_extensions=self.config['custom_extensions'],
                    max_depth=self.config['max_crawl_depth']
                )
                
                # ê²°ê³¼ ë³‘í•©
                for file_type, links in file_links.items():
                    if file_type not in all_file_links:
                        all_file_links[file_type] = []
                    all_file_links[file_type].extend(links)
                
                self.crawl_stats['urls_crawled'] += 1
                
                # ìš”ì²­ ê°„ ì§€ì—°
                if self.config['delay_between_requests'] > 0:
                    await asyncio.sleep(self.config['delay_between_requests'])
                    
            except Exception as e:
                error_msg = f"URL í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}"
                self.logger.error(error_msg)
                self.crawl_stats['errors'].append(error_msg)
        
        # ì¤‘ë³µ ë§í¬ ì œê±°
        unique_links = set()
        for file_type, links in all_file_links.items():
            unique_links.update(links)
            all_file_links[file_type] = list(set(links))
        
        self.crawl_stats['files_found'] = len(unique_links)
        self.metadata['found_links'] = all_file_links
        
        self.logger.info(f"ì´ {len(unique_links)}ê°œ íŒŒì¼ ë§í¬ ë°œê²¬")
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        if unique_links:
            self.logger.info("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
            download_results = await self.file_downloader.download_files(
                urls=list(unique_links),
                output_dir=self.config['download_dir']
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            successful_downloads = [r for r in download_results if r['success']]
            self.crawl_stats['files_downloaded'] = len(successful_downloads)
            self.crawl_stats['total_download_size'] = sum(r['size'] for r in successful_downloads)
            
            self.metadata['download_results'] = download_results
        
        self.crawl_stats['end_time'] = time.time()
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        if self.config['save_metadata']:
            await self._save_metadata()
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            'success': True,
            'stats': self.crawl_stats,
            'found_links': all_file_links,
            'download_results': self.metadata.get('download_results', []),
            'config': self.config
        }
        
        self._print_summary(result)
        return result
    
    def crawl_and_download_sync(self, 
                              urls: List[str], 
                              file_types: List[str] = None,
                              custom_extensions: Set[str] = None,
                              output_dir: str = None) -> Dict[str, Any]:
        """
        ë™ê¸° ë°©ì‹ìœ¼ë¡œ í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ
        
        Args:
            urls: í¬ë¡¤ë§í•  URL ëª©ë¡
            file_types: ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ íƒ€ì… ëª©ë¡
            custom_extensions: ì‚¬ìš©ì ì •ì˜ í™•ì¥ì
            output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
            
        Returns:
            í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ ê²°ê³¼
        """
        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        return asyncio.run(self.crawl_and_download(urls, file_types, custom_extensions, output_dir))
    
    async def find_files_only(self, 
                            urls: List[str], 
                            file_types: List[str] = None,
                            custom_extensions: Set[str] = None) -> Dict[str, List[str]]:
        """
        íŒŒì¼ ë§í¬ë§Œ ì°¾ê¸° (ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
        
        Args:
            urls: í¬ë¡¤ë§í•  URL ëª©ë¡
            file_types: ì°¾ì„ íŒŒì¼ íƒ€ì… ëª©ë¡
            custom_extensions: ì‚¬ìš©ì ì •ì˜ í™•ì¥ì
            
        Returns:
            íŒŒì¼ íƒ€ì…ë³„ ë§í¬ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("íŒŒì¼ ë§í¬ íƒì§€ ëª¨ë“œ")
        
        all_file_links = {}
        
        for url in urls:
            try:
                file_links = self.link_detector.find_file_links(
                    url=url,
                    file_types=file_types or self.config['file_types'],
                    custom_extensions=custom_extensions or self.config['custom_extensions'],
                    max_depth=self.config['max_crawl_depth']
                )
                
                # ê²°ê³¼ ë³‘í•©
                for file_type, links in file_links.items():
                    if file_type not in all_file_links:
                        all_file_links[file_type] = []
                    all_file_links[file_type].extend(links)
                    
            except Exception as e:
                self.logger.error(f"URL ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
        
        # ì¤‘ë³µ ì œê±°
        for file_type in all_file_links:
            all_file_links[file_type] = list(set(all_file_links[file_type]))
        
        return all_file_links
    
    async def download_files_from_list(self, 
                                     file_urls: List[str], 
                                     output_dir: str = None) -> List[Dict[str, Any]]:
        """
        URL ëª©ë¡ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        Args:
            file_urls: ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ URL ëª©ë¡
            output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
            
        Returns:
            ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ëª©ë¡
        """
        self.logger.info(f"{len(file_urls)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        return await self.file_downloader.download_files(
            urls=file_urls,
            output_dir=output_dir or self.config['download_dir']
        )
    
    async def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            self.metadata['crawl_info'] = {
                'timestamp': datetime.now().isoformat(),
                'stats': self.crawl_stats
            }
            
            metadata_path = Path(self.config['download_dir']) / self.config['metadata_file']
            
            # set íƒ€ì…ì„ listë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦
            serializable_metadata = self.metadata.copy()
            if 'config' in serializable_metadata:
                config_copy = serializable_metadata['config'].copy()
                if 'custom_extensions' in config_copy and isinstance(config_copy['custom_extensions'], set):
                    config_copy['custom_extensions'] = list(config_copy['custom_extensions'])
                serializable_metadata['config'] = config_copy
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _print_summary(self, result: Dict[str, Any]):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        stats = result['stats']
        duration = stats['end_time'] - stats['start_time'] if stats['end_time'] else 0
        
        print("\n" + "="*50)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        print(f"ğŸŒ í¬ë¡¤ë§í•œ URL ìˆ˜: {stats['urls_crawled']}")
        print(f"ğŸ“ ë°œê²¬í•œ íŒŒì¼ ìˆ˜: {stats['files_found']}")
        print(f"â¬‡ï¸  ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ ìˆ˜: {stats['files_downloaded']}")
        
        if stats['total_download_size'] > 0:
            size_str = self.file_downloader.format_size(stats['total_download_size'])
            print(f"ğŸ’¾ ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {size_str}")
        
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        
        if stats['errors']:
            print(f"âŒ ì˜¤ë¥˜ ìˆ˜: {len(stats['errors'])}")
        
        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
        found_links = result['found_links']
        if found_links:
            print("\nğŸ“‚ íŒŒì¼ íƒ€ì…ë³„ ë°œê²¬ ìˆ˜:")
            for file_type, links in found_links.items():
                if links:
                    print(f"   {file_type}: {len(links)}ê°œ")
        
        print("="*50)
    
    def get_supported_file_types(self) -> Dict[str, List[str]]:
        """ì§€ì›í•˜ëŠ” íŒŒì¼ íƒ€ì… ëª©ë¡ ë°˜í™˜"""
        return self.link_detector.FILE_EXTENSIONS.copy()
    
    def add_custom_extensions(self, extensions: List[str]):
        """ì‚¬ìš©ì ì •ì˜ í™•ì¥ì ì¶”ê°€"""
        for ext in extensions:
            if not ext.startswith('.'):
                ext = '.' + ext
            self.config['custom_extensions'].add(ext.lower())
    
    def set_config(self, **kwargs):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        for key, value in kwargs.items():
            if key in self.config:
                self.config[key] = value
                self.logger.info(f"ì„¤ì • ì—…ë°ì´íŠ¸: {key} = {value}")
            else:
                self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì„¤ì •: {key}")


def create_crawler_from_config_file(config_file: str) -> WebCrawler:
    """
    ì„¤ì • íŒŒì¼ì—ì„œ í¬ë¡¤ëŸ¬ ìƒì„±
    
    Args:
        config_file: JSON ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        WebCrawler ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return WebCrawler(config)
    except Exception as e:
        logging.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {config_file}: {e}")
        return WebCrawler()


# í¸ì˜ í•¨ìˆ˜ë“¤
async def quick_crawl(url: str, 
                     file_types: List[str] = None, 
                     output_dir: str = "./downloads") -> Dict[str, Any]:
    """
    ë¹ ë¥¸ í¬ë¡¤ë§ í•¨ìˆ˜
    
    Args:
        url: í¬ë¡¤ë§í•  URL
        file_types: íŒŒì¼ íƒ€ì… ëª©ë¡
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
        
    Returns:
        í¬ë¡¤ë§ ê²°ê³¼
    """
    crawler = WebCrawler()
    return await crawler.crawl_and_download([url], file_types, output_dir=output_dir)


def quick_crawl_sync(url: str, 
                    file_types: List[str] = None, 
                    output_dir: str = "./downloads") -> Dict[str, Any]:
    """
    ë¹ ë¥¸ ë™ê¸° í¬ë¡¤ë§ í•¨ìˆ˜
    """
    return asyncio.run(quick_crawl(url, file_types, output_dir))