#!/usr/bin/env python3
"""
.onion ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
from pathlib import Path
from web_crawler import WebCrawler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_onion_download():
    """ì‹¤ì œ .onion íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§… .onion ì‚¬ì´íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸í•  .onion ì‚¬ì´íŠ¸
    test_urls = [
        "http://dwkcmg5ewqvmuacunudjr7so5l2jq7wneafdzsnjllmbcec3nwu7o7id.onion"
    ]
    
    config = {
        'use_tor': True,
        'download_dir': './onion_downloads',
        'max_crawl_depth': 5,
        'file_types': ['images', 'documents', 'downloads'],
        'max_concurrent_downloads': 2,  # TorëŠ” ëŠë¦¬ë¯€ë¡œ ë™ì‹œ ë‹¤ìš´ë¡œë“œ ì œí•œ
        'enable_logging': True,
        'log_level': 'INFO'
    }
    
    try:
        # ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ ìƒì„±
        Path(config['download_dir']).mkdir(exist_ok=True)
        
        # WebCrawler ìƒì„±
        crawler = WebCrawler(config)
        
        print(f"ğŸ” í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ ì‹œì‘: {test_urls[0]}")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {config['download_dir']}")
        print(f"ğŸ¯ ëŒ€ìƒ íŒŒì¼: {', '.join(config['file_types'])}")
        print("-" * 30)
        
        # í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        result = await crawler.crawl_and_download(
            urls=test_urls,
            file_types=config['file_types'],
            output_dir=config['download_dir']
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ë‹¤ìš´ë¡œë“œ ê²°ê³¼:")
        stats = result['stats']
        
        print(f"âœ… ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: {stats['files_downloaded']}ê°œ")
        print(f"ğŸ“ ì´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {stats['total_download_size']:,} bytes")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {stats['end_time'] - stats['start_time']:.2f}ì´ˆ")
        
        if stats['errors']:
            print(f"âŒ ì˜¤ë¥˜: {len(stats['errors'])}ê°œ")
            for error in stats['errors'][:3]:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ ì¶œë ¥
                print(f"   - {error}")
        
        # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ëª©ë¡
        download_results = result.get('download_results', [])
        successful_downloads = [r for r in download_results if r['success']]
        
        if successful_downloads:
            print(f"\nğŸ“‹ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ({len(successful_downloads)}ê°œ):")
            for i, download in enumerate(successful_downloads[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                file_path = Path(download['file_path'])
                file_size = download['size']
                print(f"   {i}. {file_path.name} ({file_size:,} bytes)")
        
        print(f"\nğŸ“‚ ë‹¤ìš´ë¡œë“œ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”: {config['download_dir']}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error(f"Onion ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ .onion íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì‹¤í–‰
    asyncio.run(test_onion_download())
    
    print("\n" + "=" * 50)
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("\nğŸ’¡ íŒ:")
    print("- Tor ë„¤íŠ¸ì›Œí¬ëŠ” ì¼ë°˜ ì¸í„°ë„·ë³´ë‹¤ ëŠë¦½ë‹ˆë‹¤")
    print("- í° íŒŒì¼ ë‹¤ìš´ë¡œë“œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("- ì¼ë¶€ .onion ì‚¬ì´íŠ¸ëŠ” ì ‘ê·¼ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    main()